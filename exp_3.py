# -*- coding: utf-8 -*-
"""exp 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CQ0jDl5w278OyNYKez_UVRmJ9dX-ZiPw
"""

import pandas as pd
import numpy as np
from math import log2
from collections import Counter

# ----------------------------------------
# 1. Create Play Tennis Dataset
# ----------------------------------------

data = {
    'Outlook': ['Sunny','Sunny','Overcast','Rain','Rain','Rain','Overcast',
                'Sunny','Sunny','Rain','Sunny','Overcast','Overcast','Rain'],
    'Temperature': ['Hot','Hot','Hot','Mild','Cool','Cool','Cool',
                    'Mild','Cool','Mild','Mild','Mild','Hot','Mild'],
    'Humidity': ['High','High','High','High','Normal','Normal','Normal',
                 'High','Normal','Normal','Normal','High','Normal','High'],
    'Wind': ['Weak','Strong','Weak','Weak','Weak','Strong','Strong',
             'Weak','Weak','Weak','Strong','Strong','Weak','Strong'],
    'Play': ['No','No','Yes','Yes','Yes','No','Yes',
             'No','Yes','Yes','Yes','Yes','Yes','No']
}

df = pd.DataFrame(data)

# ----------------------------------------
# 2. Entropy Function
# ----------------------------------------

def entropy(target_column):
    counts = Counter(target_column)
    total = len(target_column)
    ent = 0
    for count in counts.values():
        p = count / total
        ent -= p * log2(p)
    return ent

# ----------------------------------------
# 3. Information Gain Function
# ----------------------------------------

def information_gain(df, attribute, target='Play'):
    total_entropy = entropy(df[target])
    values = df[attribute].unique()
    weighted_entropy = 0

    for value in values:
        subset = df[df[attribute] == value]
        weighted_entropy += (len(subset) / len(df)) * entropy(subset[target])

    return total_entropy - weighted_entropy

# ----------------------------------------
# 4. ID3 Algorithm
# ----------------------------------------

def id3(df, attributes, target='Play'):

    # If all samples belong to one class
    if len(df[target].unique()) == 1:
        return df[target].iloc[0]

    # If no attributes left
    if len(attributes) == 0:
        return df[target].mode()[0]

    # Calculate Information Gain for each attribute
    gains = [information_gain(df, attr, target) for attr in attributes]
    best_attribute = attributes[np.argmax(gains)]

    tree = {best_attribute: {}}

    for value in df[best_attribute].unique():
        subset = df[df[best_attribute] == value]

        if subset.empty:
            tree[best_attribute][value] = df[target].mode()[0]
        else:
            remaining_attributes = [attr for attr in attributes if attr != best_attribute]
            subtree = id3(subset, remaining_attributes, target)
            tree[best_attribute][value] = subtree

    return tree

# ----------------------------------------
# 5. Prediction Function
# ----------------------------------------

def predict(tree, sample):
    root = next(iter(tree))
    value = sample[root]
    subtree = tree[root][value]

    if isinstance(subtree, dict):
        return predict(subtree, sample)
    else:
        return subtree

# ----------------------------------------
# 6. Build Decision Tree
# ----------------------------------------

attributes = list(df.columns[:-1])
decision_tree = id3(df, attributes)

print("Decision Tree:")
print(decision_tree)

# ----------------------------------------
# 7. Classify New Sample
# ----------------------------------------

new_sample = {
    'Outlook': 'Sunny',
    'Temperature': 'Cool',
    'Humidity': 'High',
    'Wind': 'Strong'
}

prediction = predict(decision_tree, new_sample)
print("\nNew Sample:", new_sample)
print("Prediction:", prediction)